{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44ddad4e",
   "metadata": {},
   "source": [
    "# __Использованные материалы__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acffaa9",
   "metadata": {},
   "source": [
    "* [LightFM Github](https://github.com/lyst/lightfm)\n",
    "* [LightFM documentation](https://making.lyst.com/lightfm/docs/quickstart.html)\n",
    "* [Google recommendation systems course](https://developers.google.com/machine-learning/recommendation)\n",
    "* [Recommender system using Bayesian personalized ranking](https://towardsdatascience.com/recommender-system-using-bayesian-personalized-ranking-d30e98bba0b9)\n",
    "* [Learning to Rank Sketchfab Models with LightFM](https://www.ethanrosenthal.com/2016/11/07/implicit-mf-part-2/)\n",
    "* [How to build a Movie Recommender System in Python using LightFm](https://towardsdatascience.com/how-to-build-a-movie-recommender-system-in-python-using-lightfm-8fa49d7cbe3b)\n",
    "* [The Movies Dataset](https://www.kaggle.com/rounakbanik/the-movies-dataset/home?select=ratings_small.csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bcf4f4",
   "metadata": {},
   "source": [
    "# __Краткое введение__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2818c48b",
   "metadata": {},
   "source": [
    "__LightFM__ - это реализация на Python'е ряда популярных алгоритмов рекомендаций, включая эффективную реализацию BPR и WARP. Он прост в использовании, быстр (благодаря многопоточности) и дает высококачественные результаты. <br>\n",
    "Существуют две основные стратегии создания рекомендательных систем: \n",
    "* __Content-based Filtering__\n",
    "* __Collaborative filtering__\n",
    "\n",
    "На практике чаще всего они используются в совокупности.<br>\n",
    "<em>Далее для удобства будет использоваться термин item, который подразумевает под собой сущности, рекомендуемые системой.</em>\n",
    "\n",
    "### __Content-based Filtering__ \n",
    "Данный подход предполагает работу с метаданными пользователя, которые собираются различными способами:\n",
    "* __explicit__ - пользователь заполняет анкеты для выявление предпочтений, к примеру оценивает какой-то item по дифференцированной шкале.<br>\n",
    "* __implicit__ - все действия пользователя протоколируются для выявления предпочтений, к примеру переход по ссылками, информация о компьютере пользователя и тп.<br>\n",
    "\n",
    "### __Collaborative filtering__ \n",
    "Данный подход использует группировку пользователей и item'ов по каким-то сходствам/критериям. Будет реализоваться следующая логика \"Пользователям, которым понравился item $X$, также нравились item'ы $Y$\". Похожесть как правило определяется следующими методами:<br>\n",
    "* __Content-based__ - на основании характеристик item'ов и пользователей.<br>\n",
    "* __Transaction-based__ - на основании того, входили ли item'ы в одну транзакцию, а пользователи совершали схожие действия.<br>\n",
    "\n",
    "### Machine-learned ranking \n",
    "В __LightFM__ представлены два классических подхода MLR'а:\n",
    "* __Bayesian Personalized Ranking (BLR)__ \n",
    "* __Weighted Approximate-Rank Pairwise (WARP)__ \n",
    "\n",
    "### Bayesian Personalized Ranking \n",
    "Основная идея заключается в выборке и попарном сравнение положительных и отрицательных item'ов. Алгоритм в упрощенном виде можно представить следующим образом:\n",
    "1. Случайным образом возьмем пользователя $u$ и item $i$, который ранее был выбран пользователем, в таком случае item $i$ будет считаться <em>положительным.</em>\n",
    "2. Случайным образом возьмем item $j$, который был выбран пользователем <em>реже</em>, чем $i$ (в том числе, который пользователь никогда не выбирал), в таком случае item $j$ будет считаться <em>отрицательным.</em>\n",
    "3. Вычисляем оценку $p_{ui}$ и $p_{uj}$ пользователя $u$, а также положительного item'а $i$ и отрицательного item'а $j$ соответственно.\n",
    "4. Находим разницу между положительными и отрицательными оценками, как $x_{uij} = p_{ui} - p_{uj}.$ \n",
    "5. Пропускаем эту разницу через сигмоид и используем ее для вычисления веса для обновления всех параметров модели с помощью градиентного шага(SGD).\n",
    "\n",
    "### Weighted Approximate-Rank Pairwise\n",
    "Концепция данного подхода схожа с BPR, за исключением случаев, когда происходит градиентный шаг:\n",
    "* В BPR градиентный шаг происходит каждый раз с разницей в качестве веса.\n",
    "* WARP совершает градиентный шаг только в случае неверного предсказания (т.е. оценка отрицательного item'а больше положительного). Если предсказание было верным, то продолжаем выбирать отрицательные item'ы, пока не получим неверный прогноз или не достигнем некоторого порогового значения.\n",
    "\n",
    "Для этих целей WARP предоставляет два гиперпараметра:\n",
    "1. __Margin__ - определяет насколько ошибочным должен быть прогноз для совершения градиентного шага. \n",
    "2. __Cutoff__ - определяет сколько раз мы готовы выбирать отрицательные примеры, пытаясь получить неверное предсказание, прежде чем откажемся и перейдем к следующему пользователю.\n",
    "\n",
    "<em>Автор статьи [Learning to Rank Sketchfab Models with LightFM](https://www.ethanrosenthal.com/2016/11/07/implicit-mf-part-2/) утверждает, что на практике вероятнее всего WARP предпочтительнее для большинства рекомендательных систем, нежели BPR.</em>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6920de3c",
   "metadata": {},
   "source": [
    "# __Тестовый пример__\n",
    "Попробуем реализовать простейшую рекомендательную систему на основе [датасета, предоставляемого LightFM'ом.](https://grouplens.org/datasets/movielens/100k/)\n",
    "## __Установка зависимостей__\n",
    "### __Виртуальное окружение__\n",
    "Для его создания будет использоваться conda.\n",
    "#### Установка conda для Windows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fbbe98",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cmd\n",
    "@\"%SystemRoot%\\System32\\WindowsPowerShell\\v1.0\\powershell.exe\" -NoProfile -InputFormat None -ExecutionPolicy Bypass -Command \"[System.Net.ServicePointManager]::SecurityProtocol = 3072; iex ((New-Object System.Net.WebClient).DownloadString('https://community.chocolatey.org/install.ps1'))\" && SET \"PATH=%PATH%;%ALLUSERSPROFILE%\\chocolatey\\bin\"\n",
    "ECHO Y | choco install miniconda3 --params=\"'/AddToPath:1'\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9f6de5",
   "metadata": {},
   "source": [
    "#### Установка conda для Ubuntu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8d7434",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "sudo apt update --yes\n",
    "sudo apt upgrade --yes\n",
    "\n",
    "wget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda.sh\n",
    "bash ~/miniconda.sh -b -p ~/miniconda \n",
    "rm ~/miniconda.sh\n",
    "\n",
    "export PATH=~/miniconda/bin:$PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51258c5",
   "metadata": {},
   "source": [
    "#### Создаем и активируем виртуальное окружение c помощью команд <br>\n",
    "```\n",
    "conda create -n LightFM-env\n",
    "conda activate LightFM-env\n",
    "pip install --user ipykernel\n",
    "python -m ipykernel install --user --name=LightFM-env\n",
    "```\n",
    "#### Затем добавляем новый кернел в нотбук\n",
    "<em>По неведомым мне причинам подход к созданию виртуальной среды через conda в Windows упорно не хотел работать и активация виртуального окружения происходила только через cmd (Powershell отказывался работать). Полдня стараний зафиксить эту проблему не увенчались успехом, соответственно терминал был не доступен, поэтому дальше будет описан подход через venv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62524e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from platform import python_version\n",
    "if float(python_version()[:-2]) < 3.3: #Поскольку venv является стандартной библиотекой в Python начиная с версии 3.3.*\n",
    "    print(\"Upgrade Python to use venv library features for correct further work\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4bfe1214-d0aa-45b2-915a-7acddda78033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\python37\\lib\\site-packages (21.2.4)\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install --upgrade pip\n",
    "!{sys.executable} -m venv LightFM-env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8229ee66",
   "metadata": {},
   "source": [
    "#### Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f6e22619-8094-42a0-b03d-158dfb72b9e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft Windows [Version 10.0.19042.1165]\r\n",
      "(c) Љ®аЇ®а жЁп Њ ©Єа®б®дв (Microsoft Corporation). ‚бҐ Їа ў  § йЁйҐ­л.\r\n",
      "\r\n",
      "C:\\A.Mindset\\internship_ds\\LightFM>.\\LightFM-env\\Scripts\\activate\n",
      "\r\n",
      "(LightFM-env) C:\\A.Mindset\\internship_ds\\LightFM>python -m pip install --upgrade pip\n",
      "Collecting pip\r\n",
      "  Using cached pip-21.2.4-py3-none-any.whl (1.6 MB)\r\n",
      "Installing collected packages: pip\r\n",
      "  Attempting uninstall: pip\r\n",
      "    Found existing installation: pip 20.1.1\r\n",
      "    Uninstalling pip-20.1.1:\r\n",
      "      Successfully uninstalled pip-20.1.1\r\n",
      "Successfully installed pip-21.2.4\r\n",
      "\r\n",
      "(LightFM-env) C:\\A.Mindset\\internship_ds\\LightFM>pip install ipykernel\n",
      "Requirement already satisfied: ipykernel in c:\\a.mindset\\internship_ds\\lightfm\\lightfm-env\\lib\\site-packages (6.2.0)\r\n",
      "Requirement already satisfied: ipython<8.0,>=7.23.1 in c:\\a.mindset\\internship_ds\\lightfm\\lightfm-env\\lib\\site-packages (from ipykernel) (7.26.0)\r\n",
      "Requirement already satisfied: debugpy<2.0,>=1.0.0 in c:\\a.mindset\\internship_ds\\lightfm\\lightfm-env\\lib\\site-packages (from ipykernel) (1.4.1)\r\n",
      "Requirement already satisfied: traitlets<6.0,>=4.1.0 in c:\\a.mindset\\internship_ds\\lightfm\\lightfm-env\\lib\\site-packages (from ipykernel) (5.0.5)\r\n",
      "Requirement already satisfied: jupyter-client<8.0 in c:\\a.mindset\\internship_ds\\lightfm\\lightfm-env\\lib\\site-packages (from ipykernel) (6.1.12)\r\n",
      "Requirement already satisfied: matplotlib-inline<0.2.0,>=0.1.0 in c:\\a.mindset\\internship_ds\\lightfm\\lightfm-env\\lib\\site-packages (from ipykernel) (0.1.2)\r\n",
      "Requirement already satisfied: tornado<7.0,>=4.2 in c:\\a.mindset\\internship_ds\\lightfm\\lightfm-env\\lib\\site-packages (from ipykernel) (6.1)\r\n",
      "Requirement already satisfied: importlib-metadata<5 in c:\\a.mindset\\internship_ds\\lightfm\\lightfm-env\\lib\\site-packages (from ipykernel) (4.6.4)\r\n",
      "Requirement already satisfied: argcomplete>=1.12.3 in c:\\a.mindset\\internship_ds\\lightfm\\lightfm-env\\lib\\site-packages (from ipykernel) (1.12.3)\r\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\a.mindset\\internship_ds\\lightfm\\lightfm-env\\lib\\site-packages (from importlib-metadata<5->ipykernel) (3.5.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in c:\\a.mindset\\internship_ds\\lightfm\\lightfm-env\\lib\\site-packages (from importlib-metadata<5->ipykernel) (3.10.0.0)\r\n",
      "Requirement already satisfied: pickleshare in c:\\a.mindset\\internship_ds\\lightfm\\lightfm-env\\lib\\site-packages (from ipython<8.0,>=7.23.1->ipykernel) (0.7.5)\r\n",
      "Requirement already satisfied: colorama in c:\\a.mindset\\internship_ds\\lightfm\\lightfm-env\\lib\\site-packages (from ipython<8.0,>=7.23.1->ipykernel) (0.4.4)\r\n",
      "Requirement already satisfied: backcall in c:\\a.mindset\\internship_ds\\lightfm\\lightfm-env\\lib\\site-packages (from ipython<8.0,>=7.23.1->ipykernel) (0.2.0)\r\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in c:\\a.mindset\\internship_ds\\lightfm\\lightfm-env\\lib\\site-packages (from ipython<8.0,>=7.23.1->ipykernel) (3.0.19)\r\n",
      "Requirement already satisfied: pygments in c:\\a.mindset\\internship_ds\\lightfm\\lightfm-env\\lib\\site-packages (from ipython<8.0,>=7.23.1->ipykernel) (2.10.0)\r\n",
      "Requirement already satisfied: decorator in c:\\a.mindset\\internship_ds\\lightfm\\lightfm-env\\lib\\site-packages (from ipython<8.0,>=7.23.1->ipykernel) (5.0.9)\r\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\a.mindset\\internship_ds\\lightfm\\lightfm-env\\lib\\site-packages (from ipython<8.0,>=7.23.1->ipykernel) (0.18.0)\r\n",
      "Requirement already satisfied: setuptools>=18.5 in c:\\a.mindset\\internship_ds\\lightfm\\lightfm-env\\lib\\site-packages (from ipython<8.0,>=7.23.1->ipykernel) (47.1.0)\r\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\a.mindset\\internship_ds\\lightfm\\lightfm-env\\lib\\site-packages (from jedi>=0.16->ipython<8.0,>=7.23.1->ipykernel) (0.8.2)\r\n",
      "Requirement already satisfied: jupyter-core>=4.6.0 in c:\\a.mindset\\internship_ds\\lightfm\\lightfm-env\\lib\\site-packages (from jupyter-client<8.0->ipykernel) (4.7.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\a.mindset\\internship_ds\\lightfm\\lightfm-env\\lib\\site-packages (from jupyter-client<8.0->ipykernel) (2.8.2)\r\n",
      "Requirement already satisfied: pyzmq>=13 in c:\\a.mindset\\internship_ds\\lightfm\\lightfm-env\\lib\\site-packages (from jupyter-client<8.0->ipykernel) (22.2.1)\r\n",
      "Requirement already satisfied: pywin32>=1.0 in c:\\a.mindset\\internship_ds\\lightfm\\lightfm-env\\lib\\site-packages (from jupyter-core>=4.6.0->jupyter-client<8.0->ipykernel) (301)\r\n",
      "Requirement already satisfied: wcwidth in c:\\a.mindset\\internship_ds\\lightfm\\lightfm-env\\lib\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython<8.0,>=7.23.1->ipykernel) (0.2.5)\r\n",
      "Requirement already satisfied: six>=1.5 in c:\\a.mindset\\internship_ds\\lightfm\\lightfm-env\\lib\\site-packages (from python-dateutil>=2.1->jupyter-client<8.0->ipykernel) (1.16.0)\r\n",
      "Requirement already satisfied: ipython-genutils in c:\\a.mindset\\internship_ds\\lightfm\\lightfm-env\\lib\\site-packages (from traitlets<6.0,>=4.1.0->ipykernel) (0.2.0)\r\n",
      "\r\n",
      "(LightFM-env) C:\\A.Mindset\\internship_ds\\LightFM>python -m ipykernel install --name=LightFM-env\n",
      "Installed kernelspec LightFM-env in C:\\ProgramData\\jupyter\\kernels\\lightfm-env\r\n",
      "\r\n",
      "(LightFM-env) C:\\A.Mindset\\internship_ds\\LightFM>"
     ]
    }
   ],
   "source": [
    "%%cmd\n",
    ".\\LightFM-env\\Scripts\\activate\n",
    "python -m pip install --upgrade pip\n",
    "pip install ipykernel\n",
    "python -m ipykernel install --name=LightFM-env\n",
    "jupyter-notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "172b2a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available kernels:\n",
      "  lightfm-env    C:\\Users\\fire9\\AppData\\Roaming\\jupyter\\kernels\\lightfm-env\n",
      "  python3        c:\\a.mindset\\internship_ds\\lightfm\\lightfm-env\\share\\jupyter\\kernels\\python3\n"
     ]
    }
   ],
   "source": [
    "!jupyter kernelspec list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9928d23c",
   "metadata": {},
   "source": [
    "#### Ubuntu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348e2f6a-bd0a-4e96-975a-cadaff6f3c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "source LightFM-env/bin/activate\n",
    "python -m pip install --upgrade pip\n",
    "pip install ipykernel\n",
    "python -m ipykernel install --name=LightFM-env\n",
    "jupyter-notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780eca36",
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter kernelspec list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69949ab5",
   "metadata": {},
   "source": [
    "#### Затем добавляем новый кернел в нотбук\n",
    "### Установка библиотек для тестового примера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15e70821-e89a-4d7a-a8e3-4ed974090075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightfm\n",
      "  Using cached lightfm-1.16.tar.gz (310 kB)\n",
      "Collecting numpy\n",
      "  Downloading numpy-1.21.2-cp37-cp37m-win_amd64.whl (14.0 MB)\n",
      "Collecting scipy>=0.17.0\n",
      "  Using cached scipy-1.7.1-cp37-cp37m-win_amd64.whl (33.6 MB)\n",
      "Collecting requests\n",
      "  Downloading requests-2.26.0-py2.py3-none-any.whl (62 kB)\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-0.24.2-cp37-cp37m-win_amd64.whl (6.8 MB)\n",
      "Collecting charset-normalizer~=2.0.0\n",
      "  Downloading charset_normalizer-2.0.4-py3-none-any.whl (36 kB)\n",
      "Collecting idna<4,>=2.5\n",
      "  Downloading idna-3.2-py3-none-any.whl (59 kB)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Downloading urllib3-1.26.6-py2.py3-none-any.whl (138 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2021.5.30-py2.py3-none-any.whl (145 kB)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Using cached threadpoolctl-2.2.0-py3-none-any.whl (12 kB)\n",
      "Collecting joblib>=0.11\n",
      "  Using cached joblib-1.0.1-py3-none-any.whl (303 kB)\n",
      "Using legacy 'setup.py install' for lightfm, since package 'wheel' is not installed.\n",
      "Installing collected packages: numpy, urllib3, threadpoolctl, scipy, joblib, idna, charset-normalizer, certifi, scikit-learn, requests, lightfm\n",
      "    Running setup.py install for lightfm: started\n",
      "    Running setup.py install for lightfm: finished with status 'done'\n",
      "Successfully installed certifi-2021.5.30 charset-normalizer-2.0.4 idna-3.2 joblib-1.0.1 lightfm-1.16 numpy-1.21.2 requests-2.26.0 scikit-learn-0.24.2 scipy-1.7.1 threadpoolctl-2.2.0 urllib3-1.26.6\n",
      "Requirement already satisfied: numpy in c:\\a.mindset\\internship_ds\\lightfm\\lightfm-env\\lib\\site-packages (1.21.2)\n",
      "Requirement already satisfied: scipy in c:\\a.mindset\\internship_ds\\lightfm\\lightfm-env\\lib\\site-packages (1.7.1)\n",
      "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in c:\\a.mindset\\internship_ds\\lightfm\\lightfm-env\\lib\\site-packages (from scipy) (1.21.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install lightfm\n",
    "!pip install numpy\n",
    "!pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f7ccd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Импорт необходимых библиотек для тестового примера\n",
    "import numpy as np\n",
    "from lightfm.datasets import fetch_movielens #метод lightfm для извлечения данных фильма\n",
    "from lightfm import LightFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f6dc0096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<943x1682 sparse matrix of type '<class 'numpy.int32'>'\n",
      "\twith 49906 stored elements in COOrdinate format>\n",
      "<943x1682 sparse matrix of type '<class 'numpy.int32'>'\n",
      "\twith 5469 stored elements in COOrdinate format>\n"
     ]
    }
   ],
   "source": [
    "#Получаем данные фильма с минимальным рейтингом 4\n",
    "data = fetch_movielens(min_rating = 4.0)\n",
    "\n",
    "#Отобразим обучающий и тестовый набор\n",
    "print(repr(data['train']))\n",
    "print(repr(data['test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "114aa879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User 3\n",
      "     Known positives:\n",
      "        Seven (Se7en) (1995)\n",
      "        Contact (1997)\n",
      "        Starship Troopers (1997)\n",
      "     Recommended:\n",
      "        L.A. Confidential (1997)\n",
      "        Starship Troopers (1997)\n",
      "        Cop Land (1997)\n",
      "User 25\n",
      "     Known positives:\n",
      "        Dead Man Walking (1995)\n",
      "        Star Wars (1977)\n",
      "        Fargo (1996)\n",
      "     Recommended:\n",
      "        Fargo (1996)\n",
      "        English Patient, The (1996)\n",
      "        Contact (1997)\n",
      "User 451\n",
      "     Known positives:\n",
      "        Twelve Monkeys (1995)\n",
      "        Babe (1995)\n",
      "        Mr. Holland's Opus (1995)\n",
      "     Recommended:\n",
      "        Raiders of the Lost Ark (1981)\n",
      "        Amadeus (1984)\n",
      "        Sting, The (1973)\n"
     ]
    }
   ],
   "source": [
    "#Создадим модель\n",
    "model = LightFM(loss = 'warp')\n",
    "#Тренировка\n",
    "model.fit(data['train'], epochs=30, num_threads=2)\n",
    "\n",
    "#Рекомендательная функция\n",
    "def sample_recommendation(model, data, user_ids):\n",
    "    #Число пользователей и фильмов в обучающем наборе\n",
    "    n_users, n_items = data['train'].shape\n",
    "    for user_id in user_ids:\n",
    "    \t#Фильмы, которые уже понравились пользователям\n",
    "        known_positives = data['item_labels'][data['train'].tocsr()[user_id].indices]\n",
    "        #Предсказание фильмов, которые им понравится\n",
    "        scores = model.predict(user_id, np.arange(n_items))\n",
    "        #Сортирует результат по оценке\n",
    "        top_items = data['item_labels'][np.argsort(-scores)]\n",
    "        #Отображение результатов\n",
    "        print(\"User %s\" % user_id)\n",
    "        print(\"     Known positives:\")\n",
    "\n",
    "        for x in known_positives[:3]:\n",
    "            print(\"        %s\" % x)\n",
    "\n",
    "        print(\"     Recommended:\")\n",
    "\n",
    "        for x in top_items[:3]:\n",
    "            print(\"        %s\" % x)\n",
    "            \n",
    "sample_recommendation(model, data, [3, 25, 451])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797024bd",
   "metadata": {},
   "source": [
    "# __Работа с kaggle датасетом__\n",
    "Для этих целей возьмем датасет [The Movies Dataset](https://www.kaggle.com/rounakbanik/the-movies-dataset/home?select=keywords.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "416f7e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kaggle in c:\\a.mindset\\internship_ds\\lightfm\\lightfm-env\\lib\\site-packages (1.5.12)\n",
      "Requirement already satisfied: six>=1.10 in c:\\a.mindset\\internship_ds\\lightfm\\lightfm-env\\lib\\site-packages (from kaggle) (1.16.0)\n",
      "Requirement already satisfied: certifi in c:\\a.mindset\\internship_ds\\lightfm\\lightfm-env\\lib\\site-packages (from kaggle) (2021.5.30)\n",
      "Requirement already satisfied: python-dateutil in c:\\a.mindset\\internship_ds\\lightfm\\lightfm-env\\lib\\site-packages (from kaggle) (2.8.2)\n",
      "Requirement already satisfied: requests in c:\\a.mindset\\internship_ds\\lightfm\\lightfm-env\\lib\\site-packages (from kaggle) (2.26.0)\n",
      "Requirement already satisfied: tqdm in c:\\a.mindset\\internship_ds\\lightfm\\lightfm-env\\lib\\site-packages (from kaggle) (4.62.1)\n",
      "Requirement already satisfied: python-slugify in c:\\a.mindset\\internship_ds\\lightfm\\lightfm-env\\lib\\site-packages (from kaggle) (5.0.2)\n",
      "Requirement already satisfied: urllib3 in c:\\a.mindset\\internship_ds\\lightfm\\lightfm-env\\lib\\site-packages (from kaggle) (1.26.6)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in c:\\a.mindset\\internship_ds\\lightfm\\lightfm-env\\lib\\site-packages (from python-slugify->kaggle) (1.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\a.mindset\\internship_ds\\lightfm\\lightfm-env\\lib\\site-packages (from requests->kaggle) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\a.mindset\\internship_ds\\lightfm\\lightfm-env\\lib\\site-packages (from requests->kaggle) (3.2)\n",
      "Requirement already satisfied: colorama in c:\\a.mindset\\internship_ds\\lightfm\\lightfm-env\\lib\\site-packages (from tqdm->kaggle) (0.4.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727863fc",
   "metadata": {},
   "source": [
    "Теперь нам необходимо создать API токен на kaggle по адресу `https://www.kaggle.com/<username>/account` и поместить его в папку .kaggle, расположение которой зависит от ОС:\n",
    "* Для Windows - `C:\\Users\\<Windows-username>\\.kaggle\\kaggle.json`\n",
    "* Для Linux систем - ```~/.kaggle/kaggle.json```\n",
    "\n",
    "### Скачивание и распаковка датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bab6acb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\A.Mindset\\internship_ds\\LightFM\\LightFM-Dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0.00/228M [00:00<?, ?B/s]\n",
      "  0%|          | 1.00M/228M [00:00<00:29, 8.14MB/s]\n",
      "  2%|2         | 5.00M/228M [00:00<00:10, 22.4MB/s]\n",
      "  4%|3         | 8.00M/228M [00:00<00:09, 24.9MB/s]\n",
      "  5%|4         | 11.0M/228M [00:00<00:09, 25.0MB/s]\n",
      "  8%|8         | 19.0M/228M [00:00<00:05, 42.1MB/s]\n",
      " 12%|#2        | 28.0M/228M [00:00<00:03, 57.6MB/s]\n",
      " 15%|#5        | 35.0M/228M [00:00<00:03, 60.3MB/s]\n",
      " 18%|#7        | 41.0M/228M [00:01<00:04, 41.6MB/s]\n",
      " 20%|##        | 46.0M/228M [00:01<00:05, 34.1MB/s]\n",
      " 22%|##1       | 50.0M/228M [00:01<00:06, 30.5MB/s]\n",
      " 24%|##3       | 54.0M/228M [00:01<00:06, 28.1MB/s]\n",
      " 25%|##5       | 58.0M/228M [00:01<00:06, 26.4MB/s]\n",
      " 27%|##6       | 61.0M/228M [00:01<00:06, 26.3MB/s]\n",
      " 28%|##8       | 64.0M/228M [00:02<00:06, 27.2MB/s]\n",
      " 30%|##9       | 68.0M/228M [00:02<00:05, 28.2MB/s]\n",
      " 31%|###1      | 71.0M/228M [00:02<00:06, 27.0MB/s]\n",
      " 32%|###2      | 74.0M/228M [00:02<00:05, 27.0MB/s]\n",
      " 34%|###3      | 77.0M/228M [00:02<00:06, 26.2MB/s]\n",
      " 35%|###5      | 80.0M/228M [00:02<00:06, 25.6MB/s]\n",
      " 36%|###6      | 83.0M/228M [00:02<00:06, 25.2MB/s]\n",
      " 38%|###7      | 86.0M/228M [00:02<00:05, 24.9MB/s]\n",
      " 39%|###9      | 89.0M/228M [00:03<00:05, 24.8MB/s]\n",
      " 40%|####      | 92.0M/228M [00:03<00:05, 24.6MB/s]\n",
      " 42%|####1     | 95.0M/228M [00:03<00:05, 24.8MB/s]\n",
      " 43%|####3     | 98.0M/228M [00:03<00:05, 24.4MB/s]\n",
      " 44%|####4     | 101M/228M [00:03<00:05, 25.1MB/s] \n",
      " 46%|####5     | 104M/228M [00:03<00:05, 24.9MB/s]\n",
      " 47%|####6     | 107M/228M [00:03<00:05, 24.6MB/s]\n",
      " 48%|####8     | 110M/228M [00:04<00:05, 24.5MB/s]\n",
      " 50%|####9     | 113M/228M [00:04<00:04, 24.5MB/s]\n",
      " 51%|#####     | 116M/228M [00:04<00:04, 24.4MB/s]\n",
      " 52%|#####2    | 119M/228M [00:04<00:04, 24.5MB/s]\n",
      " 54%|#####3    | 122M/228M [00:04<00:04, 24.4MB/s]\n",
      " 55%|#####4    | 125M/228M [00:04<00:04, 23.6MB/s]\n",
      " 56%|#####6    | 128M/228M [00:04<00:04, 24.3MB/s]\n",
      " 58%|#####7    | 131M/228M [00:04<00:04, 24.9MB/s]\n",
      " 59%|#####8    | 134M/228M [00:05<00:03, 24.8MB/s]\n",
      " 60%|######    | 137M/228M [00:05<00:03, 24.7MB/s]\n",
      " 61%|######1   | 140M/228M [00:05<00:03, 24.6MB/s]\n",
      " 63%|######2   | 143M/228M [00:05<00:03, 24.5MB/s]\n",
      " 64%|######4   | 146M/228M [00:05<00:03, 24.5MB/s]\n",
      " 65%|######5   | 149M/228M [00:05<00:03, 24.5MB/s]\n",
      " 67%|######6   | 152M/228M [00:05<00:03, 24.3MB/s]\n",
      " 68%|######8   | 155M/228M [00:05<00:03, 23.5MB/s]\n",
      " 69%|######9   | 158M/228M [00:06<00:02, 25.3MB/s]\n",
      " 71%|#######   | 161M/228M [00:06<00:02, 24.9MB/s]\n",
      " 72%|#######1  | 164M/228M [00:06<00:02, 24.7MB/s]\n",
      " 73%|#######3  | 167M/228M [00:06<00:02, 24.5MB/s]\n",
      " 75%|#######4  | 170M/228M [00:06<00:02, 24.4MB/s]\n",
      " 76%|#######5  | 173M/228M [00:06<00:02, 24.3MB/s]\n",
      " 77%|#######7  | 176M/228M [00:06<00:02, 24.3MB/s]\n",
      " 79%|#######8  | 179M/228M [00:06<00:02, 25.0MB/s]\n",
      " 80%|#######9  | 182M/228M [00:07<00:01, 24.9MB/s]\n",
      " 81%|########1 | 185M/228M [00:07<00:01, 24.4MB/s]\n",
      " 83%|########2 | 188M/228M [00:07<00:01, 25.2MB/s]\n",
      " 84%|########3 | 191M/228M [00:07<00:01, 24.1MB/s]\n",
      " 85%|########5 | 194M/228M [00:07<00:01, 24.3MB/s]\n",
      " 86%|########6 | 197M/228M [00:07<00:01, 24.7MB/s]\n",
      " 88%|########7 | 200M/228M [00:07<00:01, 24.1MB/s]\n",
      " 89%|########9 | 203M/228M [00:07<00:01, 24.4MB/s]\n",
      " 90%|######### | 206M/228M [00:08<00:00, 24.5MB/s]\n",
      " 92%|#########1| 209M/228M [00:08<00:00, 24.4MB/s]\n",
      " 93%|#########3| 212M/228M [00:08<00:00, 24.5MB/s]\n",
      " 94%|#########4| 215M/228M [00:08<00:00, 24.4MB/s]\n",
      " 96%|#########5| 218M/228M [00:08<00:00, 20.0MB/s]\n",
      " 97%|#########7| 221M/228M [00:08<00:00, 22.1MB/s]\n",
      " 99%|#########8| 225M/228M [00:08<00:00, 25.7MB/s]\n",
      "100%|##########| 228M/228M [00:09<00:00, 26.4MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading the-movies-dataset.zip to C:\\A.Mindset\\internship_ds\\LightFM\\LightFM-Dataset\n",
      "\n",
      "C:\\A.Mindset\\internship_ds\\LightFM\n"
     ]
    }
   ],
   "source": [
    "from zipfile import ZipFile\n",
    "import os\n",
    "!mkdir LightFM-Dataset \n",
    "%cd .\\LightFM-Dataset\n",
    "!kaggle datasets download -d rounakbanik/the-movies-dataset\n",
    "zip_file = ZipFile('the-movies-dataset.zip')\n",
    "zip_file.extractall()\n",
    "zip_file.close()\n",
    "os.remove(\"the-movies-dataset.zip\")\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "90cfed69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available kernels:\n",
      "  lightfm-env    C:\\Users\\fire9\\AppData\\Roaming\\jupyter\\kernels\\lightfm-env\n",
      "  python3        c:\\a.mindset\\internship_ds\\lightfm\\lightfm-env\\share\\jupyter\\kernels\\python3\n"
     ]
    }
   ],
   "source": [
    "!jupyter kernelspec list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3678cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LightFM-env",
   "language": "python",
   "name": "lightfm-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
